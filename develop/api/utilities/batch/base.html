<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>aws_lambda_powertools.utilities.batch.base API documentation</title>
<meta name="description" content="Batch processing utilities" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>aws_lambda_powertools.utilities.batch.base</code></h1>
</header>
<section id="section-intro">
<p>Batch processing utilities</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-

&#34;&#34;&#34;
Batch processing utilities
&#34;&#34;&#34;
import copy
import logging
import sys
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, overload

from aws_lambda_powertools.middleware_factory import lambda_handler_decorator
from aws_lambda_powertools.utilities.batch.exceptions import BatchProcessingError, ExceptionInfo
from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord
from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord
from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord

logger = logging.getLogger(__name__)


class EventType(Enum):
    SQS = &#34;SQS&#34;
    KinesisDataStreams = &#34;KinesisDataStreams&#34;
    DynamoDBStreams = &#34;DynamoDBStreams&#34;


#
# type specifics
#
has_pydantic = &#34;pydantic&#34; in sys.modules

# For IntelliSense and Mypy to work, we need to account for possible SQS, Kinesis and DynamoDB subclasses
# We need them as subclasses as we must access their message ID or sequence number metadata via dot notation
if has_pydantic:
    from aws_lambda_powertools.utilities.parser.models import DynamoDBStreamRecordModel
    from aws_lambda_powertools.utilities.parser.models import KinesisDataStreamRecord as KinesisDataStreamRecordModel
    from aws_lambda_powertools.utilities.parser.models import SqsRecordModel

    BatchTypeModels = Optional[
        Union[Type[SqsRecordModel], Type[DynamoDBStreamRecordModel], Type[KinesisDataStreamRecordModel]]
    ]

# When using processor with default arguments, records will carry EventSourceDataClassTypes
# and depending on what EventType it&#39;s passed it&#39;ll correctly map to the right record
# When using Pydantic Models, it&#39;ll accept any subclass from SQS, DynamoDB and Kinesis
EventSourceDataClassTypes = Union[SQSRecord, KinesisStreamRecord, DynamoDBRecord]
BatchEventTypes = Union[EventSourceDataClassTypes, &#34;BatchTypeModels&#34;]
SuccessResponse = Tuple[str, Any, BatchEventTypes]
FailureResponse = Tuple[str, str, BatchEventTypes]


class BasePartialProcessor(ABC):
    &#34;&#34;&#34;
    Abstract class for batch processors.
    &#34;&#34;&#34;

    def __init__(self):
        self.success_messages: List[BatchEventTypes] = []
        self.fail_messages: List[BatchEventTypes] = []
        self.exceptions: List[ExceptionInfo] = []

    @abstractmethod
    def _prepare(self):
        &#34;&#34;&#34;
        Prepare context manager.
        &#34;&#34;&#34;
        raise NotImplementedError()

    @abstractmethod
    def _clean(self):
        &#34;&#34;&#34;
        Clear context manager.
        &#34;&#34;&#34;
        raise NotImplementedError()

    @abstractmethod
    def _process_record(self, record: dict):
        &#34;&#34;&#34;
        Process record with handler.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def process(self) -&gt; List[Tuple]:
        &#34;&#34;&#34;
        Call instance&#39;s handler for each record.
        &#34;&#34;&#34;
        return [self._process_record(record) for record in self.records]

    def __enter__(self):
        self._prepare()
        return self

    def __exit__(self, exception_type, exception_value, traceback):
        self._clean()

    def __call__(self, records: List[dict], handler: Callable):
        &#34;&#34;&#34;
        Set instance attributes before execution

        Parameters
        ----------
        records: List[dict]
            List with objects to be processed.
        handler: Callable
            Callable to process &#34;records&#34; entries.
        &#34;&#34;&#34;
        self.records = records
        self.handler = handler
        return self

    def success_handler(self, record, result: Any) -&gt; SuccessResponse:
        &#34;&#34;&#34;
        Keeps track of batch records that were processed successfully

        Parameters
        ----------
        record: Any
            record that succeeded processing
        result: Any
            result from record handler

        Returns
        -------
        SuccessResponse
            &#34;success&#34;, result, original record
        &#34;&#34;&#34;
        entry = (&#34;success&#34;, result, record)
        self.success_messages.append(record)
        return entry

    def failure_handler(self, record, exception: ExceptionInfo) -&gt; FailureResponse:
        &#34;&#34;&#34;
        Keeps track of batch records that failed processing

        Parameters
        ----------
        record: Any
            record that failed processing
        exception: ExceptionInfo
            Exception information containing type, value, and traceback (sys.exc_info())

        Returns
        -------
        FailureResponse
            &#34;fail&#34;, exceptions args, original record
        &#34;&#34;&#34;
        exception_string = f&#34;{exception[0]}:{exception[1]}&#34;
        entry = (&#34;fail&#34;, exception_string, record)
        logger.debug(f&#34;Record processing exception: {exception_string}&#34;)
        self.exceptions.append(exception)
        self.fail_messages.append(record)
        return entry


@lambda_handler_decorator
def batch_processor(
    handler: Callable, event: Dict, context: Dict, record_handler: Callable, processor: BasePartialProcessor
):
    &#34;&#34;&#34;
    Middleware to handle batch event processing

    Parameters
    ----------
    handler: Callable
        Lambda&#39;s handler
    event: Dict
        Lambda&#39;s Event
    context: Dict
        Lambda&#39;s Context
    record_handler: Callable
        Callable to process each record from the batch
    processor: PartialSQSProcessor
        Batch Processor to handle partial failure cases

    Examples
    --------
    **Processes Lambda&#39;s event with PartialSQSProcessor**

        &gt;&gt;&gt; from aws_lambda_powertools.utilities.batch import batch_processor, PartialSQSProcessor
        &gt;&gt;&gt;
        &gt;&gt;&gt; def record_handler(record):
        &gt;&gt;&gt;     return record[&#34;body&#34;]
        &gt;&gt;&gt;
        &gt;&gt;&gt; @batch_processor(record_handler=record_handler, processor=PartialSQSProcessor())
        &gt;&gt;&gt; def handler(event, context):
        &gt;&gt;&gt;     return {&#34;StatusCode&#34;: 200}

    Limitations
    -----------
    * Async batch processors

    &#34;&#34;&#34;
    records = event[&#34;Records&#34;]

    with processor(records, record_handler):
        processor.process()

    return handler(event, context)


class BatchProcessor(BasePartialProcessor):
    &#34;&#34;&#34;Process native partial responses from SQS, Kinesis Data Streams, and DynamoDB.


    Example
    -------

    ## Process batch triggered by SQS

    ```python
    import json

    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
    from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord
    from aws_lambda_powertools.utilities.typing import LambdaContext


    processor = BatchProcessor(event_type=EventType.SQS)
    tracer = Tracer()
    logger = Logger()


    @tracer.capture_method
    def record_handler(record: SQSRecord):
        payload: str = record.body
        if payload:
            item: dict = json.loads(payload)
        ...

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    @batch_processor(record_handler=record_handler, processor=processor)
    def lambda_handler(event, context: LambdaContext):
        return processor.response()
    ```

    ## Process batch triggered by Kinesis Data Streams

    ```python
    import json

    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
    from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord
    from aws_lambda_powertools.utilities.typing import LambdaContext


    processor = BatchProcessor(event_type=EventType.KinesisDataStreams)
    tracer = Tracer()
    logger = Logger()


    @tracer.capture_method
    def record_handler(record: KinesisStreamRecord):
        logger.info(record.kinesis.data_as_text)
        payload: dict = record.kinesis.data_as_json()
        ...

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    @batch_processor(record_handler=record_handler, processor=processor)
    def lambda_handler(event, context: LambdaContext):
        return processor.response()
    ```


    ## Process batch triggered by DynamoDB Data Streams

    ```python
    import json

    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
    from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord
    from aws_lambda_powertools.utilities.typing import LambdaContext


    processor = BatchProcessor(event_type=EventType.DynamoDBStreams)
    tracer = Tracer()
    logger = Logger()


    @tracer.capture_method
    def record_handler(record: DynamoDBRecord):
        logger.info(record.dynamodb.new_image)
        payload: dict = json.loads(record.dynamodb.new_image.get(&#34;item&#34;).s_value)
        # alternatively:
        # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image  # noqa: E800
        # payload = change.get(&#34;Message&#34;).raw_event -&gt; {&#34;S&#34;: &#34;&lt;payload&gt;&#34;}
        ...

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    def lambda_handler(event, context: LambdaContext):
        batch = event[&#34;Records&#34;]
        with processor(records=batch, processor=processor):
            processed_messages = processor.process() # kick off processing, return list[tuple]

        return processor.response()
    ```


    Raises
    ------
    BatchProcessingError
        When all batch records fail processing
    &#34;&#34;&#34;

    DEFAULT_RESPONSE: Dict[str, List[Optional[dict]]] = {&#34;batchItemFailures&#34;: []}

    def __init__(self, event_type: EventType, model: Optional[&#34;BatchTypeModels&#34;] = None):
        &#34;&#34;&#34;Process batch and partially report failed items

        Parameters
        ----------
        event_type: EventType
            Whether this is a SQS, DynamoDB Streams, or Kinesis Data Stream event
        model: Optional[&#34;BatchTypeModels&#34;]
            Parser&#39;s data model using either SqsRecordModel, DynamoDBStreamRecordModel, KinesisDataStreamRecord

        Exceptions
        ----------
        BatchProcessingError
            Raised when the entire batch has failed processing
        &#34;&#34;&#34;
        self.event_type = event_type
        self.model = model
        self.batch_response = copy.deepcopy(self.DEFAULT_RESPONSE)
        self._COLLECTOR_MAPPING = {
            EventType.SQS: self._collect_sqs_failures,
            EventType.KinesisDataStreams: self._collect_kinesis_failures,
            EventType.DynamoDBStreams: self._collect_dynamodb_failures,
        }
        self._DATA_CLASS_MAPPING = {
            EventType.SQS: SQSRecord,
            EventType.KinesisDataStreams: KinesisStreamRecord,
            EventType.DynamoDBStreams: DynamoDBRecord,
        }

        super().__init__()

    def response(self):
        &#34;&#34;&#34;Batch items that failed processing, if any&#34;&#34;&#34;
        return self.batch_response

    def _prepare(self):
        &#34;&#34;&#34;
        Remove results from previous execution.
        &#34;&#34;&#34;
        self.success_messages.clear()
        self.fail_messages.clear()
        self.exceptions.clear()
        self.batch_response = copy.deepcopy(self.DEFAULT_RESPONSE)

    def _process_record(self, record: dict) -&gt; Union[SuccessResponse, FailureResponse]:
        &#34;&#34;&#34;
        Process a record with instance&#39;s handler

        Parameters
        ----------
        record: dict
            A batch record to be processed.
        &#34;&#34;&#34;
        data = self._to_batch_type(record=record, event_type=self.event_type, model=self.model)
        try:
            result = self.handler(record=data)
            return self.success_handler(record=record, result=result)
        except Exception:
            return self.failure_handler(record=data, exception=sys.exc_info())

    def _clean(self):
        &#34;&#34;&#34;
        Report messages to be deleted in case of partial failure.
        &#34;&#34;&#34;

        if not self._has_messages_to_report():
            return

        if self._entire_batch_failed():
            raise BatchProcessingError(
                msg=f&#34;All records failed processing. {len(self.exceptions)} individual errors logged &#34;
                f&#34;separately below.&#34;,
                child_exceptions=self.exceptions,
            )

        messages = self._get_messages_to_report()
        self.batch_response = {&#34;batchItemFailures&#34;: messages}

    def _has_messages_to_report(self) -&gt; bool:
        if self.fail_messages:
            return True

        logger.debug(f&#34;All {len(self.success_messages)} records successfully processed&#34;)
        return False

    def _entire_batch_failed(self) -&gt; bool:
        return len(self.exceptions) == len(self.records)

    def _get_messages_to_report(self) -&gt; List[Dict[str, str]]:
        &#34;&#34;&#34;
        Format messages to use in batch deletion
        &#34;&#34;&#34;
        return self._COLLECTOR_MAPPING[self.event_type]()

    # Event Source Data Classes follow python idioms for fields
    # while Parser/Pydantic follows the event field names to the latter
    def _collect_sqs_failures(self):
        failures = []
        for msg in self.fail_messages:
            msg_id = msg.messageId if self.model else msg.message_id
            failures.append({&#34;itemIdentifier&#34;: msg_id})
        return failures

    def _collect_kinesis_failures(self):
        failures = []
        for msg in self.fail_messages:
            msg_id = msg.kinesis.sequenceNumber if self.model else msg.kinesis.sequence_number
            failures.append({&#34;itemIdentifier&#34;: msg_id})
        return failures

    def _collect_dynamodb_failures(self):
        failures = []
        for msg in self.fail_messages:
            msg_id = msg.dynamodb.SequenceNumber if self.model else msg.dynamodb.sequence_number
            failures.append({&#34;itemIdentifier&#34;: msg_id})
        return failures

    @overload
    def _to_batch_type(self, record: dict, event_type: EventType, model: &#34;BatchTypeModels&#34;) -&gt; &#34;BatchTypeModels&#34;:
        ...  # pragma: no cover

    @overload
    def _to_batch_type(self, record: dict, event_type: EventType) -&gt; EventSourceDataClassTypes:
        ...  # pragma: no cover

    def _to_batch_type(self, record: dict, event_type: EventType, model: Optional[&#34;BatchTypeModels&#34;] = None):
        if model is not None:
            return model.parse_obj(record)
        return self._DATA_CLASS_MAPPING[event_type](record)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="aws_lambda_powertools.utilities.batch.base.batch_processor"><code class="name flex">
<span>def <span class="ident">batch_processor</span></span>(<span>handler: Callable, event: Dict[~KT, ~VT], context: Dict[~KT, ~VT], record_handler: Callable, processor: <a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor">BasePartialProcessor</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Middleware to handle batch event processing</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>handler</code></strong> :&ensp;<code>Callable</code></dt>
<dd>Lambda's handler</dd>
<dt><strong><code>event</code></strong> :&ensp;<code>Dict</code></dt>
<dd>Lambda's Event</dd>
<dt><strong><code>context</code></strong> :&ensp;<code>Dict</code></dt>
<dd>Lambda's Context</dd>
<dt><strong><code>record_handler</code></strong> :&ensp;<code>Callable</code></dt>
<dd>Callable to process each record from the batch</dd>
<dt><strong><code>processor</code></strong> :&ensp;<code>PartialSQSProcessor</code></dt>
<dd>Batch Processor to handle partial failure cases</dd>
</dl>
<h2 id="examples">Examples</h2>
<p><strong>Processes Lambda's event with PartialSQSProcessor</strong></p>
<pre><code>&gt;&gt;&gt; from aws_lambda_powertools.utilities.batch import batch_processor, PartialSQSProcessor
&gt;&gt;&gt;
&gt;&gt;&gt; def record_handler(record):
&gt;&gt;&gt;     return record["body"]
&gt;&gt;&gt;
&gt;&gt;&gt; @batch_processor(record_handler=record_handler, processor=PartialSQSProcessor())
&gt;&gt;&gt; def handler(event, context):
&gt;&gt;&gt;     return {"StatusCode": 200}
</code></pre>
<h2 id="limitations">Limitations</h2>
<ul>
<li>Async batch processors</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lambda_handler_decorator
def batch_processor(
    handler: Callable, event: Dict, context: Dict, record_handler: Callable, processor: BasePartialProcessor
):
    &#34;&#34;&#34;
    Middleware to handle batch event processing

    Parameters
    ----------
    handler: Callable
        Lambda&#39;s handler
    event: Dict
        Lambda&#39;s Event
    context: Dict
        Lambda&#39;s Context
    record_handler: Callable
        Callable to process each record from the batch
    processor: PartialSQSProcessor
        Batch Processor to handle partial failure cases

    Examples
    --------
    **Processes Lambda&#39;s event with PartialSQSProcessor**

        &gt;&gt;&gt; from aws_lambda_powertools.utilities.batch import batch_processor, PartialSQSProcessor
        &gt;&gt;&gt;
        &gt;&gt;&gt; def record_handler(record):
        &gt;&gt;&gt;     return record[&#34;body&#34;]
        &gt;&gt;&gt;
        &gt;&gt;&gt; @batch_processor(record_handler=record_handler, processor=PartialSQSProcessor())
        &gt;&gt;&gt; def handler(event, context):
        &gt;&gt;&gt;     return {&#34;StatusCode&#34;: 200}

    Limitations
    -----------
    * Async batch processors

    &#34;&#34;&#34;
    records = event[&#34;Records&#34;]

    with processor(records, record_handler):
        processor.process()

    return handler(event, context)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor"><code class="flex name class">
<span>class <span class="ident">BasePartialProcessor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract class for batch processors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BasePartialProcessor(ABC):
    &#34;&#34;&#34;
    Abstract class for batch processors.
    &#34;&#34;&#34;

    def __init__(self):
        self.success_messages: List[BatchEventTypes] = []
        self.fail_messages: List[BatchEventTypes] = []
        self.exceptions: List[ExceptionInfo] = []

    @abstractmethod
    def _prepare(self):
        &#34;&#34;&#34;
        Prepare context manager.
        &#34;&#34;&#34;
        raise NotImplementedError()

    @abstractmethod
    def _clean(self):
        &#34;&#34;&#34;
        Clear context manager.
        &#34;&#34;&#34;
        raise NotImplementedError()

    @abstractmethod
    def _process_record(self, record: dict):
        &#34;&#34;&#34;
        Process record with handler.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def process(self) -&gt; List[Tuple]:
        &#34;&#34;&#34;
        Call instance&#39;s handler for each record.
        &#34;&#34;&#34;
        return [self._process_record(record) for record in self.records]

    def __enter__(self):
        self._prepare()
        return self

    def __exit__(self, exception_type, exception_value, traceback):
        self._clean()

    def __call__(self, records: List[dict], handler: Callable):
        &#34;&#34;&#34;
        Set instance attributes before execution

        Parameters
        ----------
        records: List[dict]
            List with objects to be processed.
        handler: Callable
            Callable to process &#34;records&#34; entries.
        &#34;&#34;&#34;
        self.records = records
        self.handler = handler
        return self

    def success_handler(self, record, result: Any) -&gt; SuccessResponse:
        &#34;&#34;&#34;
        Keeps track of batch records that were processed successfully

        Parameters
        ----------
        record: Any
            record that succeeded processing
        result: Any
            result from record handler

        Returns
        -------
        SuccessResponse
            &#34;success&#34;, result, original record
        &#34;&#34;&#34;
        entry = (&#34;success&#34;, result, record)
        self.success_messages.append(record)
        return entry

    def failure_handler(self, record, exception: ExceptionInfo) -&gt; FailureResponse:
        &#34;&#34;&#34;
        Keeps track of batch records that failed processing

        Parameters
        ----------
        record: Any
            record that failed processing
        exception: ExceptionInfo
            Exception information containing type, value, and traceback (sys.exc_info())

        Returns
        -------
        FailureResponse
            &#34;fail&#34;, exceptions args, original record
        &#34;&#34;&#34;
        exception_string = f&#34;{exception[0]}:{exception[1]}&#34;
        entry = (&#34;fail&#34;, exception_string, record)
        logger.debug(f&#34;Record processing exception: {exception_string}&#34;)
        self.exceptions.append(exception)
        self.fail_messages.append(record)
        return entry</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="aws_lambda_powertools.utilities.batch.base.BatchProcessor" href="#aws_lambda_powertools.utilities.batch.base.BatchProcessor">BatchProcessor</a></li>
<li><a title="aws_lambda_powertools.utilities.batch.sqs.PartialSQSProcessor" href="sqs.html#aws_lambda_powertools.utilities.batch.sqs.PartialSQSProcessor">PartialSQSProcessor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.failure_handler"><code class="name flex">
<span>def <span class="ident">failure_handler</span></span>(<span>self, record, exception: Tuple[Optional[Type[BaseException]], Optional[BaseException], Optional[traceback]]) ‑> Tuple[str, str, Union[<a title="aws_lambda_powertools.utilities.data_classes.sqs_event.SQSRecord" href="../data_classes/sqs_event.html#aws_lambda_powertools.utilities.data_classes.sqs_event.SQSRecord">SQSRecord</a>, <a title="aws_lambda_powertools.utilities.data_classes.kinesis_stream_event.KinesisStreamRecord" href="../data_classes/kinesis_stream_event.html#aws_lambda_powertools.utilities.data_classes.kinesis_stream_event.KinesisStreamRecord">KinesisStreamRecord</a>, <a title="aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event.DynamoDBRecord" href="../data_classes/dynamo_db_stream_event.html#aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event.DynamoDBRecord">DynamoDBRecord</a>, Union[Type[<a title="aws_lambda_powertools.utilities.parser.models.sqs.SqsRecordModel" href="../parser/models/sqs.html#aws_lambda_powertools.utilities.parser.models.sqs.SqsRecordModel">SqsRecordModel</a>], Type[<a title="aws_lambda_powertools.utilities.parser.models.dynamodb.DynamoDBStreamRecordModel" href="../parser/models/dynamodb.html#aws_lambda_powertools.utilities.parser.models.dynamodb.DynamoDBStreamRecordModel">DynamoDBStreamRecordModel</a>], Type[<a title="aws_lambda_powertools.utilities.parser.models.kinesis.KinesisDataStreamRecord" href="../parser/models/kinesis.html#aws_lambda_powertools.utilities.parser.models.kinesis.KinesisDataStreamRecord">KinesisDataStreamRecord</a>], None]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Keeps track of batch records that failed processing</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>record</code></strong> :&ensp;<code>Any</code></dt>
<dd>record that failed processing</dd>
<dt><strong><code>exception</code></strong> :&ensp;<code>ExceptionInfo</code></dt>
<dd>Exception information containing type, value, and traceback (sys.exc_info())</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>FailureResponse</code></dt>
<dd>"fail", exceptions args, original record</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def failure_handler(self, record, exception: ExceptionInfo) -&gt; FailureResponse:
    &#34;&#34;&#34;
    Keeps track of batch records that failed processing

    Parameters
    ----------
    record: Any
        record that failed processing
    exception: ExceptionInfo
        Exception information containing type, value, and traceback (sys.exc_info())

    Returns
    -------
    FailureResponse
        &#34;fail&#34;, exceptions args, original record
    &#34;&#34;&#34;
    exception_string = f&#34;{exception[0]}:{exception[1]}&#34;
    entry = (&#34;fail&#34;, exception_string, record)
    logger.debug(f&#34;Record processing exception: {exception_string}&#34;)
    self.exceptions.append(exception)
    self.fail_messages.append(record)
    return entry</code></pre>
</details>
</dd>
<dt id="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.process"><code class="name flex">
<span>def <span class="ident">process</span></span>(<span>self) ‑> List[Tuple[]]</span>
</code></dt>
<dd>
<div class="desc"><p>Call instance's handler for each record.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process(self) -&gt; List[Tuple]:
    &#34;&#34;&#34;
    Call instance&#39;s handler for each record.
    &#34;&#34;&#34;
    return [self._process_record(record) for record in self.records]</code></pre>
</details>
</dd>
<dt id="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.success_handler"><code class="name flex">
<span>def <span class="ident">success_handler</span></span>(<span>self, record, result: Any) ‑> Tuple[str, Any, Union[<a title="aws_lambda_powertools.utilities.data_classes.sqs_event.SQSRecord" href="../data_classes/sqs_event.html#aws_lambda_powertools.utilities.data_classes.sqs_event.SQSRecord">SQSRecord</a>, <a title="aws_lambda_powertools.utilities.data_classes.kinesis_stream_event.KinesisStreamRecord" href="../data_classes/kinesis_stream_event.html#aws_lambda_powertools.utilities.data_classes.kinesis_stream_event.KinesisStreamRecord">KinesisStreamRecord</a>, <a title="aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event.DynamoDBRecord" href="../data_classes/dynamo_db_stream_event.html#aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event.DynamoDBRecord">DynamoDBRecord</a>, Union[Type[<a title="aws_lambda_powertools.utilities.parser.models.sqs.SqsRecordModel" href="../parser/models/sqs.html#aws_lambda_powertools.utilities.parser.models.sqs.SqsRecordModel">SqsRecordModel</a>], Type[<a title="aws_lambda_powertools.utilities.parser.models.dynamodb.DynamoDBStreamRecordModel" href="../parser/models/dynamodb.html#aws_lambda_powertools.utilities.parser.models.dynamodb.DynamoDBStreamRecordModel">DynamoDBStreamRecordModel</a>], Type[<a title="aws_lambda_powertools.utilities.parser.models.kinesis.KinesisDataStreamRecord" href="../parser/models/kinesis.html#aws_lambda_powertools.utilities.parser.models.kinesis.KinesisDataStreamRecord">KinesisDataStreamRecord</a>], None]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Keeps track of batch records that were processed successfully</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>record</code></strong> :&ensp;<code>Any</code></dt>
<dd>record that succeeded processing</dd>
<dt><strong><code>result</code></strong> :&ensp;<code>Any</code></dt>
<dd>result from record handler</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>SuccessResponse</code></dt>
<dd>"success", result, original record</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def success_handler(self, record, result: Any) -&gt; SuccessResponse:
    &#34;&#34;&#34;
    Keeps track of batch records that were processed successfully

    Parameters
    ----------
    record: Any
        record that succeeded processing
    result: Any
        result from record handler

    Returns
    -------
    SuccessResponse
        &#34;success&#34;, result, original record
    &#34;&#34;&#34;
    entry = (&#34;success&#34;, result, record)
    self.success_messages.append(record)
    return entry</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="aws_lambda_powertools.utilities.batch.base.BatchProcessor"><code class="flex name class">
<span>class <span class="ident">BatchProcessor</span></span>
<span>(</span><span>event_type: <a title="aws_lambda_powertools.utilities.batch.base.EventType" href="#aws_lambda_powertools.utilities.batch.base.EventType">EventType</a>, model: Optional[ForwardRef('BatchTypeModels')] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Process native partial responses from SQS, Kinesis Data Streams, and DynamoDB.</p>
<h2 id="example">Example</h2>
<h2 id="process-batch-triggered-by-sqs">Process batch triggered by SQS</h2>
<pre><code class="language-python">import json

from aws_lambda_powertools import Logger, Tracer
from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord
from aws_lambda_powertools.utilities.typing import LambdaContext


processor = BatchProcessor(event_type=EventType.SQS)
tracer = Tracer()
logger = Logger()


@tracer.capture_method
def record_handler(record: SQSRecord):
    payload: str = record.body
    if payload:
        item: dict = json.loads(payload)
    ...

@logger.inject_lambda_context
@tracer.capture_lambda_handler
@batch_processor(record_handler=record_handler, processor=processor)
def lambda_handler(event, context: LambdaContext):
    return processor.response()
</code></pre>
<h2 id="process-batch-triggered-by-kinesis-data-streams">Process batch triggered by Kinesis Data Streams</h2>
<pre><code class="language-python">import json

from aws_lambda_powertools import Logger, Tracer
from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord
from aws_lambda_powertools.utilities.typing import LambdaContext


processor = BatchProcessor(event_type=EventType.KinesisDataStreams)
tracer = Tracer()
logger = Logger()


@tracer.capture_method
def record_handler(record: KinesisStreamRecord):
    logger.info(record.kinesis.data_as_text)
    payload: dict = record.kinesis.data_as_json()
    ...

@logger.inject_lambda_context
@tracer.capture_lambda_handler
@batch_processor(record_handler=record_handler, processor=processor)
def lambda_handler(event, context: LambdaContext):
    return processor.response()
</code></pre>
<h2 id="process-batch-triggered-by-dynamodb-data-streams">Process batch triggered by DynamoDB Data Streams</h2>
<pre><code class="language-python">import json

from aws_lambda_powertools import Logger, Tracer
from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord
from aws_lambda_powertools.utilities.typing import LambdaContext


processor = BatchProcessor(event_type=EventType.DynamoDBStreams)
tracer = Tracer()
logger = Logger()


@tracer.capture_method
def record_handler(record: DynamoDBRecord):
    logger.info(record.dynamodb.new_image)
    payload: dict = json.loads(record.dynamodb.new_image.get(&quot;item&quot;).s_value)
    # alternatively:
    # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image  # noqa: E800
    # payload = change.get(&quot;Message&quot;).raw_event -&gt; {&quot;S&quot;: &quot;&lt;payload&gt;&quot;}
    ...

@logger.inject_lambda_context
@tracer.capture_lambda_handler
def lambda_handler(event, context: LambdaContext):
    batch = event[&quot;Records&quot;]
    with processor(records=batch, processor=processor):
        processed_messages = processor.process() # kick off processing, return list[tuple]

    return processor.response()
</code></pre>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>BatchProcessingError</code></dt>
<dd>When all batch records fail processing</dd>
<dt><code>Process batch and partially report failed items</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>event_type</code></strong> :&ensp;<code><a title="aws_lambda_powertools.utilities.batch.base.EventType" href="#aws_lambda_powertools.utilities.batch.base.EventType">EventType</a></code></dt>
<dd>Whether this is a SQS, DynamoDB Streams, or Kinesis Data Stream event</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>Optional["BatchTypeModels"]</code></dt>
<dd>Parser's data model using either SqsRecordModel, DynamoDBStreamRecordModel, KinesisDataStreamRecord</dd>
</dl>
<h2 id="exceptions">Exceptions</h2>
<p>BatchProcessingError
Raised when the entire batch has failed processing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchProcessor(BasePartialProcessor):
    &#34;&#34;&#34;Process native partial responses from SQS, Kinesis Data Streams, and DynamoDB.


    Example
    -------

    ## Process batch triggered by SQS

    ```python
    import json

    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
    from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord
    from aws_lambda_powertools.utilities.typing import LambdaContext


    processor = BatchProcessor(event_type=EventType.SQS)
    tracer = Tracer()
    logger = Logger()


    @tracer.capture_method
    def record_handler(record: SQSRecord):
        payload: str = record.body
        if payload:
            item: dict = json.loads(payload)
        ...

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    @batch_processor(record_handler=record_handler, processor=processor)
    def lambda_handler(event, context: LambdaContext):
        return processor.response()
    ```

    ## Process batch triggered by Kinesis Data Streams

    ```python
    import json

    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
    from aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord
    from aws_lambda_powertools.utilities.typing import LambdaContext


    processor = BatchProcessor(event_type=EventType.KinesisDataStreams)
    tracer = Tracer()
    logger = Logger()


    @tracer.capture_method
    def record_handler(record: KinesisStreamRecord):
        logger.info(record.kinesis.data_as_text)
        payload: dict = record.kinesis.data_as_json()
        ...

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    @batch_processor(record_handler=record_handler, processor=processor)
    def lambda_handler(event, context: LambdaContext):
        return processor.response()
    ```


    ## Process batch triggered by DynamoDB Data Streams

    ```python
    import json

    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor
    from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord
    from aws_lambda_powertools.utilities.typing import LambdaContext


    processor = BatchProcessor(event_type=EventType.DynamoDBStreams)
    tracer = Tracer()
    logger = Logger()


    @tracer.capture_method
    def record_handler(record: DynamoDBRecord):
        logger.info(record.dynamodb.new_image)
        payload: dict = json.loads(record.dynamodb.new_image.get(&#34;item&#34;).s_value)
        # alternatively:
        # changes: Dict[str, dynamo_db_stream_event.AttributeValue] = record.dynamodb.new_image  # noqa: E800
        # payload = change.get(&#34;Message&#34;).raw_event -&gt; {&#34;S&#34;: &#34;&lt;payload&gt;&#34;}
        ...

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    def lambda_handler(event, context: LambdaContext):
        batch = event[&#34;Records&#34;]
        with processor(records=batch, processor=processor):
            processed_messages = processor.process() # kick off processing, return list[tuple]

        return processor.response()
    ```


    Raises
    ------
    BatchProcessingError
        When all batch records fail processing
    &#34;&#34;&#34;

    DEFAULT_RESPONSE: Dict[str, List[Optional[dict]]] = {&#34;batchItemFailures&#34;: []}

    def __init__(self, event_type: EventType, model: Optional[&#34;BatchTypeModels&#34;] = None):
        &#34;&#34;&#34;Process batch and partially report failed items

        Parameters
        ----------
        event_type: EventType
            Whether this is a SQS, DynamoDB Streams, or Kinesis Data Stream event
        model: Optional[&#34;BatchTypeModels&#34;]
            Parser&#39;s data model using either SqsRecordModel, DynamoDBStreamRecordModel, KinesisDataStreamRecord

        Exceptions
        ----------
        BatchProcessingError
            Raised when the entire batch has failed processing
        &#34;&#34;&#34;
        self.event_type = event_type
        self.model = model
        self.batch_response = copy.deepcopy(self.DEFAULT_RESPONSE)
        self._COLLECTOR_MAPPING = {
            EventType.SQS: self._collect_sqs_failures,
            EventType.KinesisDataStreams: self._collect_kinesis_failures,
            EventType.DynamoDBStreams: self._collect_dynamodb_failures,
        }
        self._DATA_CLASS_MAPPING = {
            EventType.SQS: SQSRecord,
            EventType.KinesisDataStreams: KinesisStreamRecord,
            EventType.DynamoDBStreams: DynamoDBRecord,
        }

        super().__init__()

    def response(self):
        &#34;&#34;&#34;Batch items that failed processing, if any&#34;&#34;&#34;
        return self.batch_response

    def _prepare(self):
        &#34;&#34;&#34;
        Remove results from previous execution.
        &#34;&#34;&#34;
        self.success_messages.clear()
        self.fail_messages.clear()
        self.exceptions.clear()
        self.batch_response = copy.deepcopy(self.DEFAULT_RESPONSE)

    def _process_record(self, record: dict) -&gt; Union[SuccessResponse, FailureResponse]:
        &#34;&#34;&#34;
        Process a record with instance&#39;s handler

        Parameters
        ----------
        record: dict
            A batch record to be processed.
        &#34;&#34;&#34;
        data = self._to_batch_type(record=record, event_type=self.event_type, model=self.model)
        try:
            result = self.handler(record=data)
            return self.success_handler(record=record, result=result)
        except Exception:
            return self.failure_handler(record=data, exception=sys.exc_info())

    def _clean(self):
        &#34;&#34;&#34;
        Report messages to be deleted in case of partial failure.
        &#34;&#34;&#34;

        if not self._has_messages_to_report():
            return

        if self._entire_batch_failed():
            raise BatchProcessingError(
                msg=f&#34;All records failed processing. {len(self.exceptions)} individual errors logged &#34;
                f&#34;separately below.&#34;,
                child_exceptions=self.exceptions,
            )

        messages = self._get_messages_to_report()
        self.batch_response = {&#34;batchItemFailures&#34;: messages}

    def _has_messages_to_report(self) -&gt; bool:
        if self.fail_messages:
            return True

        logger.debug(f&#34;All {len(self.success_messages)} records successfully processed&#34;)
        return False

    def _entire_batch_failed(self) -&gt; bool:
        return len(self.exceptions) == len(self.records)

    def _get_messages_to_report(self) -&gt; List[Dict[str, str]]:
        &#34;&#34;&#34;
        Format messages to use in batch deletion
        &#34;&#34;&#34;
        return self._COLLECTOR_MAPPING[self.event_type]()

    # Event Source Data Classes follow python idioms for fields
    # while Parser/Pydantic follows the event field names to the latter
    def _collect_sqs_failures(self):
        failures = []
        for msg in self.fail_messages:
            msg_id = msg.messageId if self.model else msg.message_id
            failures.append({&#34;itemIdentifier&#34;: msg_id})
        return failures

    def _collect_kinesis_failures(self):
        failures = []
        for msg in self.fail_messages:
            msg_id = msg.kinesis.sequenceNumber if self.model else msg.kinesis.sequence_number
            failures.append({&#34;itemIdentifier&#34;: msg_id})
        return failures

    def _collect_dynamodb_failures(self):
        failures = []
        for msg in self.fail_messages:
            msg_id = msg.dynamodb.SequenceNumber if self.model else msg.dynamodb.sequence_number
            failures.append({&#34;itemIdentifier&#34;: msg_id})
        return failures

    @overload
    def _to_batch_type(self, record: dict, event_type: EventType, model: &#34;BatchTypeModels&#34;) -&gt; &#34;BatchTypeModels&#34;:
        ...  # pragma: no cover

    @overload
    def _to_batch_type(self, record: dict, event_type: EventType) -&gt; EventSourceDataClassTypes:
        ...  # pragma: no cover

    def _to_batch_type(self, record: dict, event_type: EventType, model: Optional[&#34;BatchTypeModels&#34;] = None):
        if model is not None:
            return model.parse_obj(record)
        return self._DATA_CLASS_MAPPING[event_type](record)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor">BasePartialProcessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="aws_lambda_powertools.utilities.batch.base.BatchProcessor.DEFAULT_RESPONSE"><code class="name">var <span class="ident">DEFAULT_RESPONSE</span> : Dict[str, List[Optional[dict]]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="aws_lambda_powertools.utilities.batch.base.BatchProcessor.response"><code class="name flex">
<span>def <span class="ident">response</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Batch items that failed processing, if any</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def response(self):
    &#34;&#34;&#34;Batch items that failed processing, if any&#34;&#34;&#34;
    return self.batch_response</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor">BasePartialProcessor</a></b></code>:
<ul class="hlist">
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.failure_handler" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.failure_handler">failure_handler</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.process" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.process">process</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.success_handler" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.success_handler">success_handler</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="aws_lambda_powertools.utilities.batch.base.EventType"><code class="flex name class">
<span>class <span class="ident">EventType</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EventType(Enum):
    SQS = &#34;SQS&#34;
    KinesisDataStreams = &#34;KinesisDataStreams&#34;
    DynamoDBStreams = &#34;DynamoDBStreams&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="aws_lambda_powertools.utilities.batch.base.EventType.DynamoDBStreams"><code class="name">var <span class="ident">DynamoDBStreams</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="aws_lambda_powertools.utilities.batch.base.EventType.KinesisDataStreams"><code class="name">var <span class="ident">KinesisDataStreams</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="aws_lambda_powertools.utilities.batch.base.EventType.SQS"><code class="name">var <span class="ident">SQS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="aws_lambda_powertools.utilities.batch" href="index.html">aws_lambda_powertools.utilities.batch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="aws_lambda_powertools.utilities.batch.base.batch_processor" href="#aws_lambda_powertools.utilities.batch.base.batch_processor">batch_processor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor">BasePartialProcessor</a></code></h4>
<ul class="">
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.failure_handler" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.failure_handler">failure_handler</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.process" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.process">process</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.success_handler" href="#aws_lambda_powertools.utilities.batch.base.BasePartialProcessor.success_handler">success_handler</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="aws_lambda_powertools.utilities.batch.base.BatchProcessor" href="#aws_lambda_powertools.utilities.batch.base.BatchProcessor">BatchProcessor</a></code></h4>
<ul class="">
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BatchProcessor.DEFAULT_RESPONSE" href="#aws_lambda_powertools.utilities.batch.base.BatchProcessor.DEFAULT_RESPONSE">DEFAULT_RESPONSE</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.BatchProcessor.response" href="#aws_lambda_powertools.utilities.batch.base.BatchProcessor.response">response</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="aws_lambda_powertools.utilities.batch.base.EventType" href="#aws_lambda_powertools.utilities.batch.base.EventType">EventType</a></code></h4>
<ul class="">
<li><code><a title="aws_lambda_powertools.utilities.batch.base.EventType.DynamoDBStreams" href="#aws_lambda_powertools.utilities.batch.base.EventType.DynamoDBStreams">DynamoDBStreams</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.EventType.KinesisDataStreams" href="#aws_lambda_powertools.utilities.batch.base.EventType.KinesisDataStreams">KinesisDataStreams</a></code></li>
<li><code><a title="aws_lambda_powertools.utilities.batch.base.EventType.SQS" href="#aws_lambda_powertools.utilities.batch.base.EventType.SQS">SQS</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>